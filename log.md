# 100 Days Of ML Code - Daily Log

Introduction: me and my junior son @catfield123 are participating this challenge together. 

### Day 0: July 11, 2018

**Today's Progress:**
- We have decided following topics as top-interesting:
    - One-Shot Learning architectures, MANN.
    - Mixed precision ANN training capabilities
     
- Continued our dive into following papers and presentations:
    - [One-shot Learning with Memory-Augmented Neural Networks](https://arxiv.org/abs/1605.06065v1)
    - [Neural Turing Machines](https://arxiv.org/pdf/1410.5401.pdf)
    - [Neural networks with external
       memory](https://pdfs.semanticscholar.org/presentation/3896/fb6dafe8de209cf5f4375ce8c351dcf43c3c.pdf)

**Hours spent:** 1

**Thoughts:** We just started and still looking for real ML task to implement in this challenge. Still studying papers so far.

**TODO:**
- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)
- [Mixed Precision Training of Convolutional Neural Networks using Integer Operations](https://arxiv.org/abs/1802.00930)

### Day 1: July 12, 2018

**Today's Progress:**

- Reading papers we've got clear following ideas
    - Both types of MANN were invented in DeepMind:
        - NTM (Neural Turing Machine) by Alex Graves et. al. 2014
        - DNC (Differential Neural Computer) by Alex Graves et. al. 2016
    - Being a much improved direct descendant of NTM, the DNC adds:
        - new attention mechanisms (addressing via soft/hard attention)
        - dynamically allocated memory
        - memory links

**Hours spent:** 1.0

**Thoughts:** Is there one-shot learning implementation which is based on DNC instead of NTM?

**TODO:**
- [DNC Nature 2016 article](https://www.gwern.net/docs/rl/2016-graves.pdf)
- [DNC slides](http://people.idsia.ch/~rupesh/rnnsymposium2016/slides/graves.pdf)
- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)
- [Mixed Precision Training of Convolutional Neural Networks using Integer Operations](https://arxiv.org/abs/1802.00930)
 
